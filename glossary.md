---
description: List of deep learning terms
---

# Glossary

<table data-view="cards"><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><strong>Activation Function</strong></td><td>Introduces non-linearity into a neural network, enabling it to learn complex patterns.</td></tr><tr><td><strong>Backpropagation</strong></td><td>Algorithm used to compute the gradients of the loss function with respect to each parameter.</td></tr><tr><td><strong>Batch Normalisation</strong></td><td>Normalises the inputs of each layer to improve stability and speed of training.</td></tr><tr><td><strong>Cost Function</strong></td><td>Another term for loss function, measures prediction error.</td></tr><tr><td><strong>Dropout</strong></td><td>Regularisation technique that randomly ignores selected neurons during training to prevent overfitting.</td></tr><tr><td><strong>Epoch</strong></td><td>A single pass through the entire training dataset.</td></tr><tr><td><strong>Gradient Descent</strong></td><td>Optimisation algorithm that minimises the loss function by iteratively updating model parameters.</td></tr><tr><td><strong>Learning Rate</strong></td><td>Hyperparameter controlling the update step size during gradient descent.</td></tr><tr><td><strong>Loss Function</strong></td><td>Function measuring how well predictions match actual target values.</td></tr><tr><td><strong>Optimiser</strong></td><td>Algorithm used to adjust the weights and biases of the network to minimise the loss function.</td></tr><tr><td><strong>Overfitting</strong></td><td>When a model learns the training data too well, including noise, and performs poorly on new data.</td></tr><tr><td><strong>Regularisation</strong></td><td>Techniques used to prevent overfitting by adding a penalty to the loss function.</td></tr><tr><td><strong>Vanishing Gradient Problem</strong></td><td>Issue where gradients become too small, inhibiting learning in deep networks.</td></tr><tr><td><strong>Weight Initialisation</strong></td><td>Method for setting the initial values of weights before training.</td></tr><tr><td><strong>Weight Decay</strong></td><td>Regularisation technique that adds a penalty to the loss function proportional to the magnitude of the weights.</td></tr><tr><td><strong>Training Set</strong></td><td>Subset of the dataset used to train the model.</td></tr><tr><td><strong>Validation Set</strong></td><td>Subset of the dataset used to provide an unbiased evaluation of the model during training.</td></tr><tr><td><strong>Test Set</strong></td><td>Subset of the dataset used to evaluate the final model performance.</td></tr></tbody></table>
